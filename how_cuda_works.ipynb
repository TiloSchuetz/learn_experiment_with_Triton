{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb355280",
   "metadata": {},
   "source": [
    "## How CUDA and GPU parallelization works\n",
    "\n",
    "This notebook works as a short-summary of key-terms and main concepts that are relevant for understanding GPUs and parallelization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d8c1a1",
   "metadata": {},
   "source": [
    "### 1. Summary of key terms\n",
    "\n",
    "**FLOPS** floating-point operations per second\n",
    "-> measures the throughput of a GPU. Theoretical peak throughput = (number of cores) × (instructions per core per cycle) × (clock frequency)\n",
    "\n",
    "**Core**: execution unit capable of performing operations\n",
    "\n",
    "**Clock cycle**: one “tick” of the processor’s clock\n",
    "\n",
    "**Fused Multiply-Add (FMA)**: some GPUs can do an addition and multiplication within on clock cycle\n",
    "\n",
    "**Clock Frequency**: measured in Heartz = cycles per second. 1.5 GHz = 1.5 billion cycles per second\n",
    "\n",
    "**Memory Bandwidth**: rate at which data can be transferred between GPU memory (VRAM) and GPU compute units. Usually measures in GB/S (gigabytes per second)\n",
    "\n",
    "**Transistor**: on and off switches that can build logic gates, which can be combined into processors\n",
    "\n",
    "**Data Caching**: saving a small copy of data close to the processor, such that it doesn't need to be fetched from RAM all the time\n",
    "\n",
    "**Flow Control**: ensures data flow without bottlenecks or overflow. In GPUs/CPUs it decides when to stall, flush, or forward instructions so the pipeline runs smoothly\n",
    "\n",
    "**Streaming Multiprocessors**: building blocks of NVIDIA GPUs that are basically small processors. Has its own cores + memory and is able to run thousands of threads at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47755cf",
   "metadata": {},
   "source": [
    "### 2. GPU architecture & key concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa89e9df",
   "metadata": {},
   "source": [
    "##### 2.1 CPU vs. GPU\n",
    "\n",
    "<figure>\n",
    "  <img src=\"./images/gpu-devotes-more-transistors-to-data-processing.png\" alt=\"Architecture comparison CPU vs. GPU\" width=\"500\">\n",
    "  <figcaption><i>Figure 1: Architecture comparison CPU vs. GPU</i></figcaption>\n",
    "</figure>\n",
    "\n",
    "In a GPU much more transistors are used for processing data rather than data caching and flow control! This improves parallelism a lot, but comes at the cost of memory latency. In a CPU higher memory latency would be a big problem, as tasks are executed sequentially. But a GPU can do other computations on different threads, as some threads wait for data. This effectively 'hides' the memory latency. At least for tasks that a GPU is designed for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52284480",
   "metadata": {},
   "source": [
    "##### 2.2 Scalable Programming Model\n",
    "\n",
    "**Main requirement**: Scaling cores or multiple GPUs to achieve faster performance only works, if the software allows for scaled parallelism, meaning that more compute resources allow for more parallel task executions. This requires that the task at hand can be divided into sub-tasks that can be independently executed in any order.\n",
    "\n",
    "\n",
    "<figure>\n",
    "  <img src=\"./images/automatic-scalability.png\" alt=\"Automatic Scalability with more GPUs\" width=\"500\">\n",
    "  <figcaption><i>Figure 2: Automatic Scalability with more GPUs</i></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9716c083",
   "metadata": {},
   "source": [
    "##### Sources\n",
    "- CUDA Guide -> https://docs.nvidia.com/cuda/cuda-c-programming-guide/#introduction"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
